version: 0.2

# run-as: Linux-user-name

env:
  shell: bash
        
phases:
  install:
    commands:
      # Install mountpoint
      - wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm
      - sudo yum -y install ./mount-s3.rpm
  pre_build:
    commands:
      # Pull the image from the registry. This is faster than building from
      # scratch, especially when cached
      - echo 'Pulling the Docker image...'
      - aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/i2m6n0b5
      - docker pull public.ecr.aws/i2m6n0b5/nitelite-pipeline:latest
      # Mount the S3 buckets
      - ./aws/mount_buckets.sh
      # If there's checkpointed output, retrieve it
      - ./aws/get_checkpoint.sh
      # Run the test suite
      - ./aws/run_tests.sh
  build:
    commands:
      # Run the pipeline. We set the timeout to 1 hr prior to the max
      # allowed by AWS (480m = 8h), so that there's time to copy the data
      - timeout --preserve-status 15m ./bin/pipeline.sh -c ${CONFIG_FILEPATH} -d /data  -f ./aws/docker-compose.yaml --mount_code
  post_build:
    commands:
      # Check output
      - echo 'Output:'; ls -R /data/output
      # Set up name for the artifact in the destination bucket
      - BASENAME=$(basename ${CONFIG_FILEPATH})
      - DIR=${BASENAME%.*}
      - SUBDIR=${FLIGHT_ID}_$(date +%Y-%m-%d_%H-%M-%S)
      - echo "Artifacts will be stored in s3://nitelite.pipeline-output/$DIR/$SUBDIR"

artifacts:
  base-directory: /data/output
  files:
    - "**/*"
  discard-paths: no
  name: $DIR/$SUBDIR